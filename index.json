[{"authors":["admin"],"categories":null,"content":"Hi! I am an undergraduate at Northwestern University studying computer science and math, and I identify with the natural language processing and computer audition communities. My current research work includes definition modeling for word compounds, generating natural language captions for audio, and some tinkering in music generation. I\u0026rsquo;ve recently been thinking a lot about developing automatic measures of model confidence and how they can be used to enable different training techniques and incorporate human input. I have been super fortunate to work with Professor Doug Downey, Professor Bryan Pardo, and Dr. Prem Seetharaman, who have been the kindest of mentors and completely formative to my intellectual interests.\n","date":1572566400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576802920,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alisawuffles.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! I am an undergraduate at Northwestern University studying computer science and math, and I identify with the natural language processing and computer audition communities. My current research work includes definition modeling for word compounds, generating natural language captions for audio, and some tinkering in music generation. I\u0026rsquo;ve recently been thinking a lot about developing automatic measures of model confidence and how they can be used to enable different training techniques and incorporate human input.","tags":null,"title":"Alisa Liu","type":"authors"},{"authors":["alex"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576802920,"objectID":"d7149a99f41440e55ea517c3fb5d3c99","permalink":"https://alisawuffles.github.io/authors/alex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex/","section":"authors","summary":"","tags":null,"title":"Alexander Fang","type":"authors"},{"authors":null,"categories":null,"content":"I really enjoy making study guides! Here are a couple I'm especially proud of, and I will update this post as time goes on.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.\nMath 312: Number Theory One of my favorite classes (and subjects) of all time. I shared this with the class on Piazza (with permission from Dr. Troupe, of course) and passed it on to many students who would take the course after me. I still hear from people who use it, which makes me really happy.\nMath 307: Applications of Linear Algebra More of a formula sheet than a study guide.\n","date":1573257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575097629,"objectID":"14b7ef143889783a1f832b97903693a4","permalink":"https://alisawuffles.github.io/post/study-guides/","publishdate":"2019-11-09T00:00:00Z","relpermalink":"/post/study-guides/","section":"post","summary":"I really enjoy making study guides! Here are a couple I'm especially proud of, and I will update this post as time goes on.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.\nMath 312: Number Theory One of my favorite classes (and subjects) of all time. I shared this with the class on Piazza (with permission from Dr.","tags":null,"title":"Some study guides I've made","type":"post"},{"authors":null,"categories":null,"content":"Developing a transformer model that generates natural language descriptions of audio clips using the recently released Clotho dataset, the first audio caption dataset with captions collected without accompanying video.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574748522,"objectID":"ab2e6fd2ea63548427746d9d4783b359","permalink":"https://alisawuffles.github.io/project/audio-captioning/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/audio-captioning/","section":"project","summary":"Generating natural language captions for audio clips","tags":["research","ongoing","audio"],"title":"Audio captioning","type":"project"},{"authors":["Alisa Liu","Alexander Fang"],"categories":null,"content":" Deep learning has become the state-of-the-art approach for music generation, but these networks require massive data. For music generation systems on a specific music domain, such as Bach chorales, Mozart sonatas, or Beatles hits, training data is limited to the real work of musicians during their lifetime. Researchers commonly approach this problem by artificially augmenting the dataset with synthetic data, a technique known as dataset augmentation [1]. In the musical domain, a natural way is to transpose music examples in all keys, or to segment examples into many shorter fragments. However, these all suffer musical limitations: the original key signature is deliberately chosen by the composer and typically associated with certain emotions and expressive qualities, and segmenting examples prevents a network from being able to learn the long-term coherence of human-written music.\nWe propose an alternative method for data augmentation, which we call augmentative generation. After training a base model on the original dataset, we generate some output from the model, filter it by a hand-crafted score function, and feed it back to the model as new training data to update the model. These generate-update iterations can be continued until it stops improving the model.\nWe try this method with chorale generation in the style of J.S. Bach, using the DeepBach model [2]. For Bach chorale generation, gold-standard data is inherently limited to the 389 chorales Bach wrote in his lifetime. While this is considered a large output for a single form for a composer, 389 examples are often not enough to train machine learning models. Moreover, the unique sound quality of each key signature is especially important in Baroque compositions, which are are written with the equal temperament tuning system in mind, giving each key a distinctive character. This makes transposition an undesirable method of dataset augmentation and motivates our proposed solution.\nWhile augmentative generation can be applied to any generation system where training data is limited, the score function must be handcrafted for each domain. We argue that this is a good thing, since a limitation of deep learning techniques compared to handcrafted models is that they do not offer direct ways of controlling generation. But given that we know some things about what widely accepted music (like Bach chorales or Mozart sonatas) should sound like, we believe that a handcrafted function allows musicians to incorporate important domain knowledge into the music generation task, while still using the representational capacity of a neural network. That is, rather than enforcing things about our output, we can enforce things about the input that our model is trained on.\nProposed method First we train the base model on the original 351 Bach chorales. Compared to the original DeepBach paper, we do not use any transpositions of Bach chorales.\nThen, we perform a series of generation-train updates. In each iteration, we generate $N$ chorales and score each one. We set our threshold as the score of the lowest-scoring Bach chorale, with the intuition that every real Bach chorale should be selected by our score function. We update the model by training on the selected examples for $M$ epochs.\n   Augmentative generation training procedure applied to DeepBach   DeepBach We apply our method to DeepBach, an architecture that was designed for the generation of Bach chorales. It combines two recurrent networks (LSTMs) and two feedforward networks. One recurrent network sums up past information, one recurrent network sums up future information, and a non-recurrent network represents notes occurring at the same time. The outputs of the three models are merged and passed as the input of a final feedforward neural network, whose output activation function is softmax. This architecture is replicated four times, one for each voice in a chorale. Generation is done by sampling, using a pseudo-Gibbs sampling incremental and iterative algorithm.    DeepBach’s neural network architecture for prediction of the soprano voice   We use the same data representation as in the original DeepBach paper. In particular, notes are encoded as MIDI pitches and discretized into sixteenth notes. A hold symbol “\u0026ndash;” is used to indicate whether the preceding note is held. The fermata symbol for Bach chorales is explicitly considered in the metadata to help produce structure and coherent phrases.\nScore function The score function determines which generated chorales are used to update the model. Given a chorale, the score function should output a value that captures how much it looks like real Bach chorales. Our basic method is to compare feature distributions between a given chorale and real Bach chorales. (To be clear, a distribution is a histogram of counts, normalized by the total number of notes.) For each feature $f$, we use the Wasserstein metric to measure the distance between the distribution $P_{c}^f$ of the given chorale $c$ and the ground-truth distribution $P_{\\text{Bach}}^f$ over real Bach chorales. The resulting value, $Wass\\left(P_{c}^f,P_{\\text{Bach}}^f\\right)$, is weighted by $w^f$. The overall score is therefore a weighted sum of these distances over all features $f$. That is, $$S( c )=\\sum_{f\\in\\text{features}} Wass\\left(P_{c}^f,P_{\\text{Bach}}^f\\right)\\cdot w^f$$ Since low distances represent high similarity between a given chorale and real Bach chorales, we subtract $S( c )$ from a large constant so that higher scores correspond to better chorales.\nThe weights $w^f$ are carefully handpicked in an iterative process where we modify the weights, judge its correlation with our own perception of chorale quality, and modify the weights further. For each feature we take into account the natural value range of $Wass\\left(P_{c}^f,P_{\\text{Bach}}^f\\right)$, its empirical usefulness in distinguishing chorale quality, and its overall musical importance to a good chorale.\nThe features we use in the score function are notes, rhythm, interval, parallel errors, and other voice leading errors. We describe these next.\nNotes The note distribution is the distribution of notes in scale degrees. The ground-truth note distribution for Bach chorales in major and Bach chorales in minor are calculated separately. They are shown below.\n   Distribution of notes (in scale degrees) over Bach chorales   For an example, consider the following generated chorale $c$ and its note distribution $P_{c}^\\text{note}$.\n    The score of a chorale generated by our base model      Distribution of notes (in scale degrees) for an example chorale   Since this chorale is in G major, the chorale note distribution is compared to the ground-truth major note distribution to obtain $Wass\\left(P_{c}^\\text{note},P_{\\text{Bach}}^{\\text{note}\\,(M)}\\right)$.\nRhythm The rhythm distribution is the distribution of note lengths in units of quarter notes.\nFrom the ground-truth rhythm distribution shown below, we see that Bach chorales are fairly saturated with quarter notes and eighth notes.\n   Distribution of note lengths (in units of quarter notes) over Bach chorales   Interval The directed interval distribution is the distribution of directed interval sizes.\nGiven the ground-truth interval distribution shown below, we should expect many small intervals for more singable lines, compared to leaps that are large or dissonant. For easier visualization, we include only intervals that have at least a $0.0025$ chance of happening 1.\n   Distribution of intervals over Bach chorales   Parallel errors The parallel errors distribution is the distribution of parallel fifths and parallel octaves errors.\n   Distribution of parallel errors over Bach chorales   However, note here that what matters is not only the ratio of parallel 5ths to parallel octaves, but the absolute count of these errors relative to the total number of notes. Therefore, $w^\\text{parallel}$ includes a term $$\\frac{\\text{error to note ratio of chorale }c}{\\text{error to note ratio of Bach}}$$\nThis term is large if the given chorale has a large error to note ratio compared to real Bach chorales, thereby penalizing the given chorale.\nOther errors The other errors distribution is the distribution of hidden 5ths, hidden octaves, voice overlaps, voice crossings, and voice range violations. We separate this feature from the parallel errors because whereas we do not expect to see many parallel fifths and octaves, we do expect a large number of hidden fifths and octaves.\n   Distribution of other errors over Bach chorales   Similarly to the previous feature, $w^\\text{errors}$ includes a term to account for the absolute count of errors in addition to the relative probabilities in the distribution.\nExperiments In our experiments, we first show that the score function effectively distinguishes real Bach chorales from generations. Then we carry out human evaluations to compare generations of three models: our model trained through augmentative generation, our base model, and the model in the original DeepBach paper.\nEvaluating the score function Because the score function measures how similar a given chorale is to real Bach chorales, a simple evaluation method is to validate that real Bach chorales score higher than generated chorales. Therefore, we plot the distribution of scores for real Bach chorales and chorales generated by our base model.    The score distribution for Bach chorales and generated chorales   These plots show that Bach chorales score much better on average than generated chorales, indicating that the score function serves its purpose of measuring similarity to real Bach chorales. We also see that generated chorales display high variation in quality, which demonstrates the importance of an external evaluation metric for generated chorales.\nNote that based on our score threshold, the generated chorales which which would be selected can be visualized as those that \u0026ldquo;overlap\u0026rdquo; with real chorales in the score distribution.\nModel details We updated the model for $43$ iterations. Each iteration, we generated $50$ training examples for consideration. We train on the selected chorales for $2$ epochs. Of note, after this process our model saw less than one-fifth of the training data used in the original DeepBach paper, which consists of Bach chorales and their transpositions.\nEvaluation In the paired discrimination task, participants with music training are presented with pairs of audio clips where one is a real Bach chorale and one is generated output. The generated output comes from one of three models: our model trained through augmentative generation on Bach chorales and high-quality generations, our base model trained on only Bach chorales, and the model in the original DeepBach paper trained on Bach chorales and their transpositions. Participants are told that not all generated outputs are produced by the same model. We then compare human accuracy at detecting generated music for each model.\nFor preliminary experiments, we have $n=1$ participant with a conservatory degree in piano performance. We presented 30 pairs of audio samples, using $30$ Bach chorales and $10$ randomly selected generations from each model. Each audio sample represents $4$ measures of music, rendered with MuseScore at $80$ beats per minute.\n   Model Accuracy     DeepBach (trained on Bach chorales) 0.90   DeepBach (trained on Bach chorales + transpositions) 0.90   DeepBach (trained on Bach chorales + augmentative generation) 0.90    Unfortunately our experiments were conducted under the grueling demands of finals week, and not yet extensive enough to be discriminative. Future work is discussed below.\nMusic samples See if you can discriminate between Bach chorales and generated chorales yourself! In each of the following pairs, one chorale is composed by Bach and one is generated. The answers are here 2.\n The generated chorale comes from DeepBach (trained on Bach chorales), which is our base model. Your browser does not support the audio element.  Your browser does not support the audio element.   The generated chorale comes from DeepBach (trained on Bach chorales + transpositions), from the original DeepBach paper. Your browser does not support the audio element.  Your browser does not support the audio element.   The generated chorale comes from DeepBach (trained on Bach chorales + augmentative generation), which is our contribution. Your browser does not support the audio element.  Your browser does not support the audio element.    Upcoming work and ideas  Every handful of generate-train updates, do another pass through the original Bach chorale training data. This way, we ensure that the model does not \u0026ldquo;forget\u0026rdquo; what gold-standard data looks like. Show (hopefully) that the score distribution of generated chorales is increasing over time and becoming more similar to the score distribution over Bach chorales. This shows that according to the score function, generations are improving as generate-train updates go on. Currently, there is no method of determining when to discontinue generate-train iterations. We need to implement code to evaluate the validation loss at the end of every iteration, in order to stop when training on more generations is no longer improving model performance on validation data. Conduct a much more extensive version of the human evaluation. In addition to measuring discrimination accuracy, we intend to measure decision time as an indicator of confidence. This is motivated by an observation we made in our preliminary experiment: even though our participant displayed the same performance for each model, his amount hesitation varied significantly (sometimes, he did not even listen to the full audio sample; other times, he deliberated thoughtfully and made a \u0026ldquo;guess\u0026rdquo;.) We hope to show that our model causes longer decision times, suggesting that its generations are more challenging to distinguish from real Bach chorales. Moreover, we hope to show that decision time correlates positively with the chorale score: that is, the higher-scoring a chorale is, the longer it takes a trained listener to determine whether it is real or generated.  References  Jean-Pierre Briot and François Pachet. \u0026ldquo;Music Generation by Deep Learning – Challenges and Directions.\u0026rdquo;Neural Computing and Applications, 2018. Gaetan Hadjeres, Francois Pachet, Frank Nielsen. “DeepBach: a Steerable Model for Bach chorales generation.” The International Conference on Machine Learning (ICML), 2016.   The interval types that were excluded are: M6, m6, d1, m-6, d-4, M-6, m7, d4, d-7, M7, d5, m-7, M9, A5, A4, M-7, M-9, A8, P11, M10, P12, A2, d-3, A-4, m10, d7, P15, A-5, and d3. ^ (1) first chorale is real. (2) second chorale is real. (3) first chorale is real. ^   ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576802920,"objectID":"5b51174e7df864c1a7c10e2389885074","permalink":"https://alisawuffles.github.io/project/deepbach/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/deepbach/","section":"project","summary":"Using heuristic evaluations of generated music to train a Bach chorale generator on high-quality generations","tags":["research","class project","ongoing","music"],"title":"Augmentative generation for Bach chorales","type":"project"},{"authors":null,"categories":null,"content":"Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”). We identified a large pool of noun compounds in text using a POS-tagging rule, and we are currently incorporating human-authored definitions for these unlabeled noun compounds to evaluate the impact of active learning on generation tasks.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574750563,"objectID":"00e0b2b997f11bab3817414c7c89d96b","permalink":"https://alisawuffles.github.io/project/noun-compounds/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/noun-compounds/","section":"project","summary":"Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”)","tags":["research","ongoing","nlp"],"title":"Definition modeling for noun compounds","type":"project"},{"authors":null,"categories":null,"content":"Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering. We derived a confidence measure based on the clusterability of the embedding space which approximates the separation quality without ground-truth comparison.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575097629,"objectID":"d37ff57d947f77965df9552739958496","permalink":"https://alisawuffles.github.io/project/ensemble/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/ensemble/","section":"project","summary":"Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering.","tags":["research","audio"],"title":"Ensemble model for audio source separation","type":"project"},{"authors":["Alisa Liu","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576377481,"objectID":"24c73dc0b65350f6d17d1115dbdc9450","permalink":"https://alisawuffles.github.io/publication/ensemble/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ensemble/","section":"publication","summary":"Ensemble model for audio source separation, using a confidence measure to mediate among domain-specific models","tags":["source separation","deep learning","performance prediction","ensemble methods","deep clustering"],"title":"Model Selection for Deep Audio Source Separation via Clustering Analysis","type":"publication"},{"authors":["Ruimin Zhu","Thanapon Noraset","Alisa Liu","Wenxin Jiang","Doug Downey"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576380737,"objectID":"cc50e81bd32d61f2343cb06b26a34d89","permalink":"https://alisawuffles.github.io/publication/multidef/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/multidef/","section":"publication","summary":"Definition generation for multiple senses of a word","tags":["definition modeling"],"title":"Multi-sense Definition Modeling using Word Sense Decompositions","type":"publication"},{"authors":["Michael Chen","Mike D'arcy","Alisa Liu","Jared Fernandez","Doug Downey"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576377481,"objectID":"b9966e05ba0d9a68862f41435e4eef65","permalink":"https://alisawuffles.github.io/publication/codah/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/codah/","section":"publication","summary":"An adversarially-constructed dataset for common sense QA, collected from Northwestern ML students!","tags":["common sense","dataset","question-answering"],"title":"CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense","type":"publication"},{"authors":null,"categories":null,"content":"Produced an adversarially generated commonsense question-answer dataset, using a novel question acquisition procedure where workers author questions designed to target weaknesses of state-of-the-art neural QA systems.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574750563,"objectID":"458a37348903afb0500e28f0d89a9c59","permalink":"https://alisawuffles.github.io/project/codah/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/project/codah/","section":"project","summary":"Introduced an adversarially generated commonsense question-answer dataset","tags":["research","nlp"],"title":"Common sense QA dataset","type":"project"},{"authors":null,"categories":null,"content":" Word embeddings are vector representations of words that form the foundation for many NLP tasks. While they have been shown to capture semantic similarities and encode analogical relations between words, these comparison tasks only evaluate an embedding’s information indirectly. In contrast, the task of generating natural language definitions from word embeddings provides a more transparent view of the information captured in a word embedding.\nHowever, existing definition models are limited in their ability to generate accurate definitions for different senses of the same word. In this paper, we introduce a new method that enables definition modeling for multiple senses. Some selected examples are shown below.\n   Selected examples of the multi-sense definition model\u0026rsquo;s outputs   Quantitative analysis of error types In my project, my goal was to better understand the settings under which our model succeeds and fails.\nTo evaluate the generated definitions, we manually label outputs as one of four categories: ${\\bf I}$ the output is correct; ${\\bf II}$ the output has either a syntax/fluency error or a semantic issue, but not both; ${\\bf III}$ the output has both a syntactic and semantic error but is not completely wrong; and ${\\bf IV}$ where the output is completely wrong. When evaluating precision and recall, the four labeling categories are given scores $1.0$, $0.6$, $0.3$, and $0.0$ respectively. A breakdown of the error types is shown below.\n   A breakdown of error types, and the percentage of each in our best model vs. the baseline. For each error type, one example is provided. An output can have more than one error type.   I investigated whether certain attributes of words and atoms are predictive of model performance. The word attributes we considered included the ranking of word frequency, the number of ground-truth definitions of the word, the semantic diversity of ground-truth definitions, and the word embedding norm. Atom attributes included the atom weight after decomposition and the part of speech of the atom. We used logistic regression with these attributes to predict two different output variables: individual error types and the score from $0$ to $1$. For predicting the score, we trained logistic regression to minimize the cross-entropy between the model output and the score (i.e., we treated the non-0/1 score labels as probabilities). We performed 5-fold validation, where atoms belonging to the same word must always be in the same fold.\nI was unable to predict the individual error labels with accuracy above baseline, which suggests the attributes were not good predictors given the scale of data we had available, and demonstrates that definition generation is still a challenging problem. However, the score prediction model predicts the score with $0.48$ loss, compared to the $0.53$ baseline, using only atom weight as an attribute, which is a significant predictor with p-value $\u0026lt; 0.01$. We hypothesize that this is because atoms with greater weight are more likely to represent more dominant senses that are easier to define. In fact, the atoms with the top $10\\%$ in weight have an average score of $0.35$, substantially higher than the average score of $0.19$ across all atoms.\nIn a separate analysis of part of speech, I found that adjectives achieve the highest average score. This is likely the result of having the shortest average output definition length, making the redundancy label (the most common syntactical failure mode) rare. (Adjectives have the redundancy label $3\\%$ of the time, compared to $15\\%$ for nouns.)\n   POS average score average length     adj 0.39 5.7   noun 0.30 10.1   adv 0.32 7.2   verb 0.31 5.9    This work in definition modeling for single words led me to my next project, where I generate definitions for word compounds!\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576380737,"objectID":"08aa2c89e1cec131a6eca62ff3d19fc0","permalink":"https://alisawuffles.github.io/project/multidef/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/multidef/","section":"project","summary":"Evaluated the settings under which a multi-sense definition modeling system succeeded and failed","tags":["research","nlp"],"title":"Analysis of error types in multi-sense definition generation","type":"project"},{"authors":["Dexter Everett","Alisa Liu","Jenny Pan"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573591768,"objectID":"f3e9e9bef1f6b746451371a680c860d9","permalink":"https://alisawuffles.github.io/publication/crispr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/crispr/","section":"publication","summary":"Comparison of the portrayal of CRISPR/Cas9 in mainstream media and academic literature. (From my pre-med, baby-research days.)","tags":["literature review"],"title":"Comparison of Discourse Surrounding CRISPR/Cas9 in the Media and Peer-Reviewed Literature","type":"publication"}]