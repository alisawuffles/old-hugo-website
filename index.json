[{"authors":["admin"],"categories":null,"content":"Hi! I am currently an undergraduate research intern on the MOSAIC team at AI2. I graduated this Spring from Northwestern University with majors in computer science and math. There, I did work in the fields of natural language processing and computer audition. I have been super fortunate to work with Professor Doug Downey, Professor Bryan Pardo, and Dr. Prem Seetharaman, who have been the kindest of mentors and completely formative to my intellectual interests.\nI am super excited to join the University of Washington in Fall 2020 as a PhD student in the CSE department.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1591832377,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alisawuffles.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! I am currently an undergraduate research intern on the MOSAIC team at AI2. I graduated this Spring from Northwestern University with majors in computer science and math. There, I did work in the fields of natural language processing and computer audition. I have been super fortunate to work with Professor Doug Downey, Professor Bryan Pardo, and Dr. Prem Seetharaman, who have been the kindest of mentors and completely formative to my intellectual interests.","tags":null,"title":"Alisa Liu","type":"authors"},{"authors":["Alex Fang","Alisa Liu","Gaëtan Hadjeres","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593622170,"objectID":"0f1eef010e82530575783b5905dcbb2e","permalink":"https://alisawuffles.github.io/publication/grading_function/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grading_function/","section":"publication","summary":"An automatic, interpretable, and musically-motivated grading function for Bach chorales","tags":["automatic evaluation","Bach chorales","music generation","deep learning","domain knowledge"],"title":"Bach or Mock? A Grading Function for Chorales in the Style of J.S. Bach","type":"publication"},{"authors":["Alisa Liu","Alexander Fang","Gaëtan Hadjeres","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593622709,"objectID":"02d47a6a3ec7f5380370e75bbc3968d2","permalink":"https://alisawuffles.github.io/publication/aug_gen/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/aug_gen/","section":"publication","summary":"presenting a generative data augmentation method for music generation systems on a resource-constrained domain","tags":["music generation","data augmentation","deep learning","domain knowledge"],"title":"Incorporating Music Knowledge in Continual Dataset Augmentation for Music Generation","type":"publication"},{"authors":null,"categories":null,"content":"I really enjoy making study guides! Here are a couple I'm especially proud of, and I hope they are useful resources for other students.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.\nMath 312: Number Theory One of my favorite classes (and subjects) of all time. I shared this with the class and passed it on to many students who would take the course after me. I still hear from people who use it, which makes me really happy.\nMath 320: Real Analysis This class ⁠makes robust everything we know and love from calculus. In this study guide I try to outline approaches to common problems and provide examples of functions with interesting properties.\nMath 307: Applications of Linear Algebra More of a formula sheet than a study guide.\n","date":1573257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577408995,"objectID":"14b7ef143889783a1f832b97903693a4","permalink":"https://alisawuffles.github.io/post/study-guides/","publishdate":"2019-11-09T00:00:00Z","relpermalink":"/post/study-guides/","section":"post","summary":"I really enjoy making study guides! Here are a couple I'm especially proud of, and I hope they are useful resources for other students.\nEECS 396: Statistical Machine Learning The professor shared the topic being tested by every problem on the final, so I organized course content around it.\nMath 312: Number Theory One of my favorite classes (and subjects) of all time. I shared this with the class and passed it on to many students who would take the course after me.","tags":null,"title":"Some study guides I've made","type":"post"},{"authors":null,"categories":null,"content":"Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”). We identified a large pool of noun compounds in text using a POS-tagging rule, and we are currently incorporating human-authored definitions for these unlabeled noun compounds to evaluate the impact of active learning on generation tasks.\n","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574750563,"objectID":"00e0b2b997f11bab3817414c7c89d96b","permalink":"https://alisawuffles.github.io/project/noun-compounds/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/project/noun-compounds/","section":"project","summary":"Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”)","tags":["research","ongoing","nlp"],"title":"Definition modeling for noun compounds","type":"project"},{"authors":null,"categories":null,"content":"Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering. We derived a confidence measure based on the clusterability of the embedding space which approximates the separation quality without ground-truth comparison.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575097629,"objectID":"d37ff57d947f77965df9552739958496","permalink":"https://alisawuffles.github.io/project/ensemble/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/ensemble/","section":"project","summary":"Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering.","tags":["research","audio"],"title":"Ensemble model for audio source separation","type":"project"},{"authors":["Alisa Liu","Alex Fang","Gaëtan Hadjeres","Prem Seetharaman","Bryan Pardo"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593621952,"objectID":"24c73dc0b65350f6d17d1115dbdc9450","permalink":"https://alisawuffles.github.io/publication/ensemble/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ensemble/","section":"publication","summary":"Ensemble model for audio source separation, using a confidence measure to mediate among domain-specific models","tags":["source separation","deep learning","performance prediction","ensemble methods","deep clustering"],"title":"Model Selection for Deep Audio Source Separation via Clustering Analysis","type":"publication"},{"authors":["Ruimin Zhu","Thanapon Noraset","Alisa Liu","Wenxin Jiang","Doug Downey"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576380737,"objectID":"cc50e81bd32d61f2343cb06b26a34d89","permalink":"https://alisawuffles.github.io/publication/multidef/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/multidef/","section":"publication","summary":"Definition generation for multiple senses of a word","tags":["definition modeling"],"title":"Multi-sense Definition Modeling using Word Sense Decompositions","type":"publication"},{"authors":["Michael Chen","Mike D'arcy","Alisa Liu","Jared Fernandez","Doug Downey"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576377481,"objectID":"b9966e05ba0d9a68862f41435e4eef65","permalink":"https://alisawuffles.github.io/publication/codah/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/codah/","section":"publication","summary":"An adversarially-constructed dataset for common sense QA, collected from Northwestern ML students!","tags":["common sense","dataset","question-answering"],"title":"CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense","type":"publication"},{"authors":null,"categories":null,"content":"Produced an adversarially generated commonsense question-answer dataset, using a novel question acquisition procedure where workers author questions designed to target weaknesses of state-of-the-art neural QA systems.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574750563,"objectID":"458a37348903afb0500e28f0d89a9c59","permalink":"https://alisawuffles.github.io/project/codah/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/project/codah/","section":"project","summary":"Introduced an adversarially generated commonsense question-answer dataset","tags":["research","nlp"],"title":"Common sense QA dataset","type":"project"},{"authors":null,"categories":null,"content":"Word embeddings are vector representations of words that form the foundation for many NLP tasks. While they have been shown to capture semantic similarities and encode analogical relations between words, these comparison tasks only evaluate an embedding’s information indirectly. In contrast, the task of generating natural language definitions from word embeddings provides a more transparent view of the information captured in a word embedding.\nHowever, existing definition models are limited in their ability to generate accurate definitions for different senses of the same word. In this paper, we introduce a new method that enables definition modeling for multiple senses. Some selected examples are shown below.\n   Selected examples of the multi-sense definition model\u0026rsquo;s outputs   Quantitative analysis of error types In my project, my goal was to better understand the settings under which our model succeeds and fails.\nTo evaluate the generated definitions, we manually label outputs as one of four categories: ${\\bf I}$ the output is correct; ${\\bf II}$ the output has either a syntax/fluency error or a semantic issue, but not both; ${\\bf III}$ the output has both a syntactic and semantic error but is not completely wrong; and ${\\bf IV}$ where the output is completely wrong. When evaluating precision and recall, the four labeling categories are given scores $1.0$, $0.6$, $0.3$, and $0.0$ respectively. A breakdown of the error types is shown below.\n   A breakdown of error types, and the percentage of each in our best model vs. the baseline. For each error type, one example is provided. An output can have more than one error type.   I investigated whether certain attributes of words and atoms are predictive of model performance. The word attributes we considered included the ranking of word frequency, the number of ground-truth definitions of the word, the semantic diversity of ground-truth definitions, and the word embedding norm. Atom attributes included the atom weight after decomposition and the part of speech of the atom. We used logistic regression with these attributes to predict two different output variables: individual error types and the score from $0$ to $1$. For predicting the score, we trained logistic regression to minimize the cross-entropy between the model output and the score (i.e., we treated the non-0/1 score labels as probabilities). We performed 5-fold validation, where atoms belonging to the same word must always be in the same fold.\nI was unable to predict the individual error labels with accuracy above baseline, which suggests the attributes were not good predictors given the scale of data we had available, and demonstrates that definition generation is still a challenging problem. However, the score prediction model predicts the score with $0.48$ loss, compared to the $0.53$ baseline, using only atom weight as an attribute, which is a significant predictor with p-value $\u0026lt; 0.01$. We hypothesize that this is because atoms with greater weight are more likely to represent more dominant senses that are easier to define. In fact, the atoms with the top $10%$ in weight have an average score of $0.35$, substantially higher than the average score of $0.19$ across all atoms.\nIn a separate analysis of part of speech, I found that adjectives achieve the highest average score. This is likely the result of having the shortest average output definition length, making the redundancy label (the most common syntactical failure mode) rare. (Adjectives have the redundancy label $3%$ of the time, compared to $15%$ for nouns.)\n   POS average score average length     adj 0.39 5.7   noun 0.30 10.1   adv 0.32 7.2   verb 0.31 5.9    This work in definition modeling for single words led me to my next project, where I generate definitions for word compounds!\n","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576380737,"objectID":"08aa2c89e1cec131a6eca62ff3d19fc0","permalink":"https://alisawuffles.github.io/project/multidef/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/project/multidef/","section":"project","summary":"Evaluated the settings under which a multi-sense definition modeling system succeeded and failed","tags":["research","nlp"],"title":"Analysis of error types in multi-sense definition generation","type":"project"},{"authors":["Dexter Everett*","Alisa Liu*","Jenny Pan*"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577408995,"objectID":"f3e9e9bef1f6b746451371a680c860d9","permalink":"https://alisawuffles.github.io/publication/crispr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/crispr/","section":"publication","summary":"Comparison of the portrayal of CRISPR/Cas9 in mainstream media and academic literature. (From my pre-med, baby-research days.)","tags":["literature review"],"title":"Comparison of Discourse Surrounding CRISPR/Cas9 in the Media and Peer-Reviewed Literature","type":"publication"}]