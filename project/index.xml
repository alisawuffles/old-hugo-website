<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Alisa Liu</title>
    <link>https://alisawuffles.github.io/project/</link>
      <atom:link href="https://alisawuffles.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 01 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alisawuffles.github.io/img/icon-32.png</url>
      <title>Projects</title>
      <link>https://alisawuffles.github.io/project/</link>
    </image>
    
    <item>
      <title>Audio captioning</title>
      <link>https://alisawuffles.github.io/project/audio-captioning/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/audio-captioning/</guid>
      <description>&lt;p&gt;Developing a transformer model that generates natural language descriptions of audio clips using the recently released &lt;a href=&#34;https://arxiv.org/abs/1910.09387&#34; target=&#34;_blank&#34;&gt;Clotho dataset&lt;/a&gt;, the first audio caption dataset with captions collected without accompanying video.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmentative generation for Bach chorales</title>
      <link>https://alisawuffles.github.io/project/deepbach/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/deepbach/</guid>
      <description>

&lt;p&gt;Deep learning has become the state-of-the-art approach for music generation, but these networks require massive data. For music generation systems on a specific music domain, such as Bach chorales, Mozart sonatas, or Beatles hits, training data is limited to the real work of musicians during their lifetime. Researchers commonly approach this problem by artificially augmenting the dataset with synthetic data, a technique known as &lt;em&gt;dataset augmentation&lt;/em&gt; &lt;a href=&#34;#challenges_directions_cite&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;. In the musical domain, a natural way is to transpose music examples in all keys, or to segment examples into many shorter fragments. However, these all suffer musical limitations: the original key signature is deliberately chosen by the composer and typically associated with certain emotions and expressive qualities, and segmenting examples prevents a network from being able to learn the long-term coherence of human-written music.&lt;/p&gt;

&lt;p&gt;We propose an alternative method for data augmentation, which we call &lt;em&gt;augmentative generation&lt;/em&gt;. After training a base model on the original dataset, we generate some output from the model, filter it by a hand-crafted score function, and feed it back to the model as new training data to update the model. These generate-update iterations can be continued until it stops improving the model.&lt;/p&gt;

&lt;p&gt;We try this method with chorale generation in the style of J.S. Bach, using the DeepBach model &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;. For Bach chorale generation, gold-standard data is inherently limited to the 389 chorales Bach wrote in his lifetime. While this is considered a large output for a single form for a composer, 389 examples are often not enough to train machine learning models. Moreover, the unique sound quality of each key signature is especially important in Baroque compositions, which are are written with the equal temperament tuning system in mind, giving each key a distinctive character. This makes transposition an undesirable method of dataset augmentation and motivates our proposed solution.&lt;/p&gt;

&lt;p&gt;While augmentative generation can be applied to any generation system where training data is limited, the score function must be handcrafted for each domain. We argue that this is a good thing, since a limitation of deep learning techniques compared to handcrafted models is that they do not offer direct ways of controlling generation. But given that we know some things about what widely accepted music (like Bach chorales or Mozart sonatas) should sound like, we believe that a handcrafted function allows musicians to incorporate important domain knowledge into the music generation task, while still using the representational capacity of a neural network. That is, rather than enforcing things about our output, we can enforce things about the input that our model is trained on.&lt;/p&gt;

&lt;h1 id=&#34;proposed-method&#34;&gt;Proposed method&lt;/h1&gt;

&lt;p&gt;First we train the base model on the original 351 Bach chorales. Compared to the original DeepBach paper, we do not use any transpositions of Bach chorales.&lt;/p&gt;

&lt;p&gt;Then, we perform a series of generation-train updates. In each iteration, we generate $N$ chorales and score each one. We set our threshold as the score of the lowest-scoring Bach chorale, with the intuition that every real Bach chorale should be selected by our score function. We update the model by training on the selected examples for $M$ epochs.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/architecture.png&#34; data-caption=&#34;Augmentative generation training procedure applied to DeepBach&#34;&gt;
&lt;img src=&#34;../../img/architecture.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Augmentative generation training procedure applied to DeepBach
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;deepbach&#34;&gt;DeepBach&lt;/h2&gt;

&lt;p&gt;We apply our method to &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;DeepBach&lt;/a&gt;, an architecture that was designed for the generation of Bach chorales. It combines two recurrent networks (LSTMs) and two feedforward networks. One recurrent network sums up past information, one recurrent network sums up future information, and a non-recurrent network represents notes occurring at the same time. The outputs of the three models are merged and passed as the input of a final feedforward neural network, whose output activation function is softmax. This architecture is replicated four times, one for each voice in a chorale. Generation is done by sampling, using a pseudo-Gibbs sampling incremental and iterative algorithm.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/deepbach_architecture.png&#34; data-caption=&#34;DeepBach’s neural network architecture for prediction of the soprano voice&#34;&gt;
&lt;img src=&#34;../../img/deepbach_architecture.png&#34; alt=&#34;&#34; width=&#34;30%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    DeepBach’s neural network architecture for prediction of the soprano voice
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We use the same data representation as in the original DeepBach paper. In particular, notes are encoded as MIDI pitches and discretized into sixteenth notes. A hold symbol “&amp;ndash;” is used to indicate whether the preceding note is held. The fermata symbol for Bach chorales is explicitly considered in the metadata to help produce structure and coherent phrases.&lt;/p&gt;

&lt;h2 id=&#34;score-function&#34;&gt;Score function&lt;/h2&gt;

&lt;p&gt;The score function determines which generated chorales are used to update the model. Given a chorale, the score function should output a value that captures how much it looks like real Bach chorales. Our basic method is to compare feature distributions between a given chorale and real Bach chorales. (To be clear, a distribution is a histogram of counts, normalized by the total number of notes.) For each feature $f$, we use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wasserstein_metric&#34; target=&#34;_blank&#34;&gt;Wasserstein metric&lt;/a&gt; to measure the distance between the distribution $P_{c}^f$ of the given chorale $c$ and the ground-truth distribution $P_{\text{Bach}}^f$ over real Bach chorales. The resulting value, $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$, is weighted by $w^f$. The overall score is therefore a weighted sum of these distances over all features $f$. That is,
$$S( c )=\sum_{f\in\text{features}} Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)\cdot w^f$$
Since low distances represent high similarity between a given chorale and real Bach chorales, we subtract $S( c )$ from a large constant so that higher scores correspond to better chorales.&lt;/p&gt;

&lt;p&gt;The weights $w^f$ are carefully handpicked in an iterative process where we modify the weights, judge its correlation with our own perception of chorale quality, and modify the weights further. For each feature we take into account the natural value range of $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$, its empirical usefulness in distinguishing chorale quality, and its overall musical importance to a good chorale.&lt;/p&gt;

&lt;p&gt;The features we use in the score function are notes, rhythm, interval, parallel errors, and other voice leading errors. We describe these next.&lt;/p&gt;

&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;The note distribution is the distribution of notes in scale degrees. The ground-truth note distribution for Bach chorales in major and Bach chorales in minor are calculated separately. They are shown below.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/note_distributions.png&#34; data-caption=&#34;Distribution of notes (in scale degrees) over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/note_distributions.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of notes (in scale degrees) over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For an example, consider the following generated chorale $c$ and its note distribution $P_{c}^\text{note}$.&lt;/p&gt;

&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex.png&#34; data-caption=&#34;The score of a chorale generated by our base model&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex.png&#34; alt=&#34;&#34; width=&#34;70%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score of a chorale generated by our base model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex_distribution.png&#34; data-caption=&#34;Distribution of notes (in scale degrees) for an example chorale&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of notes (in scale degrees) for an example chorale
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Since this chorale is in G major, the chorale note distribution is compared to the ground-truth major note distribution to obtain $Wass\left(P_{c}^\text{note},P_{\text{Bach}}^{\text{note}\,(M)}\right)$.&lt;/p&gt;

&lt;h3 id=&#34;rhythm&#34;&gt;Rhythm&lt;/h3&gt;

&lt;p&gt;The rhythm distribution is the distribution of note lengths in units of quarter notes.&lt;/p&gt;

&lt;p&gt;From the ground-truth rhythm distribution shown below, we see that Bach chorales are fairly saturated with quarter notes and eighth notes.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/rhythm_distribution.png&#34; data-caption=&#34;Distribution of note lengths (in units of quarter notes) over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/rhythm_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of note lengths (in units of quarter notes) over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;interval&#34;&gt;Interval&lt;/h3&gt;

&lt;p&gt;The directed interval distribution is the distribution of directed interval sizes.&lt;/p&gt;

&lt;p&gt;Given the ground-truth interval distribution shown below, we should expect many small intervals for more singable lines, compared to leaps that are large or dissonant. For easier visualization, we include only intervals that have at least a $0.0025$ chance of happening &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/directed_interval_distribution.png&#34; data-caption=&#34;Distribution of intervals over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/directed_interval_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of intervals over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h3 id=&#34;parallel-errors&#34;&gt;Parallel errors&lt;/h3&gt;

&lt;p&gt;The parallel errors distribution is the distribution of parallel fifths and parallel octaves errors.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/parallel_error_distribution.png&#34; data-caption=&#34;Distribution of parallel errors over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/parallel_error_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of parallel errors over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;However, note here that what matters is not only the ratio of parallel 5ths to parallel octaves, but the absolute count of these errors relative to the total number of notes. Therefore, $w^\text{parallel}$ includes a term
$$\frac{\text{error to note ratio of chorale }c}{\text{error to note ratio of Bach}}$$&lt;/p&gt;

&lt;p&gt;This term is large if the given chorale has a large error to note ratio compared to real Bach chorales, thereby penalizing the given chorale.&lt;/p&gt;

&lt;h3 id=&#34;other-errors&#34;&gt;Other errors&lt;/h3&gt;

&lt;p&gt;The other errors distribution is the distribution of hidden 5ths, hidden octaves, voice overlaps, voice crossings, and voice range violations. We separate this feature from the parallel errors because whereas we do not expect to see many parallel fifths and octaves, we do expect a large number of hidden fifths and octaves.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/error_distribution.png&#34; data-caption=&#34;Distribution of other errors over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/error_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of other errors over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Similarly to the previous feature, $w^\text{errors}$ includes a term to account for the absolute count of errors in addition to the relative probabilities in the distribution.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;In our experiments, we first show that the score function effectively distinguishes real Bach chorales from generations. Then we carry out human evaluations to compare generations of three models: our model trained through augmentative generation, our base model, and the model in the original DeepBach paper.&lt;/p&gt;

&lt;h2 id=&#34;evaluating-the-score-function&#34;&gt;Evaluating the score function&lt;/h2&gt;

&lt;p&gt;Because the score function measures how similar a given chorale is to real Bach chorales, a simple evaluation method is to validate that real Bach chorales score higher than generated chorales. Therefore, we plot the distribution of scores for real Bach chorales and chorales generated by our base model.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/score_distribution.png&#34; data-caption=&#34;The score distribution for Bach chorales and generated chorales&#34;&gt;
&lt;img src=&#34;../../img/score_distribution.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score distribution for Bach chorales and generated chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;

These plots show that Bach chorales score much better on average than generated chorales, indicating that the score function serves its purpose of measuring similarity to real Bach chorales. We also see that generated chorales display high variation in quality, which demonstrates the importance of an external evaluation metric for generated chorales.&lt;/p&gt;

&lt;p&gt;Note that based on our score threshold, the generated chorales which which would be selected can be visualized as those that &amp;ldquo;overlap&amp;rdquo; with real chorales in the score distribution.&lt;/p&gt;

&lt;h2 id=&#34;model-details&#34;&gt;Model details&lt;/h2&gt;

&lt;p&gt;We updated the model for $43$ iterations. Each iteration, we generated $50$ training examples for consideration. We train on the selected chorales for $2$ epochs. Of note, after this process our model saw less than one-fifth of the training data used in the original DeepBach paper, which consists of Bach chorales and their transpositions.&lt;/p&gt;

&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;In the paired discrimination task, participants with music training are presented with pairs of audio clips where one is a real Bach chorale and one is generated output. The generated output comes from one of three models: our model trained through augmentative generation on Bach chorales and high-quality generations, our base model trained on only Bach chorales, and the model in the original DeepBach paper trained on Bach chorales and their transpositions. Participants are told that not all generated outputs are produced by the same model. We then compare human accuracy at detecting generated music for each model.&lt;/p&gt;

&lt;p&gt;For preliminary experiments, we have $n=1$ participant with a conservatory degree in piano performance. We presented 30 pairs of audio samples, using $30$ Bach chorales and $10$ randomly selected generations from each model. Each audio sample represents $4$ measures of music, rendered with MuseScore at $80$ beats per minute.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + transpositions)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + augmentative generation)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Unfortunately our experiments were conducted under the grueling demands of finals week, and not yet extensive enough to be discriminative. Future work is discussed below.&lt;/p&gt;

&lt;h1 id=&#34;music-samples&#34;&gt;Music samples&lt;/h1&gt;

&lt;p&gt;See if you can discriminate between Bach chorales and generated chorales yourself! In each of the following pairs, one chorale is composed by Bach and one is generated. The answers are here &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales), which is our base model.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/4_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/4_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + transpositions), from the original DeepBach paper.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/3_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/3_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + augmentative generation), which is our contribution.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/6_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/6_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;upcoming-work-and-ideas&#34;&gt;Upcoming work and ideas&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Every handful of generate-train updates, do another pass through the original Bach chorale training data. This way, we ensure that the model does not &amp;ldquo;forget&amp;rdquo; what gold-standard data looks like.&lt;/li&gt;
&lt;li&gt;Show (hopefully) that the score distribution of generated chorales is increasing over time and becoming more similar to the score distribution over Bach chorales. This shows that according to the score function, generations are improving as generate-train updates go on.&lt;/li&gt;
&lt;li&gt;Currently, there is no method of determining when to discontinue generate-train iterations. We need to implement code to evaluate the validation loss at the end of every iteration, in order to stop when training on more generations is no longer improving model performance on validation data.&lt;/li&gt;
&lt;li&gt;Conduct a much more extensive version of the human evaluation. In addition to measuring discrimination accuracy, we intend to measure decision time as an indicator of confidence. This is motivated by an observation we made in our preliminary experiment: even though our participant displayed the same performance for each model, his amount hesitation varied significantly (sometimes, he did not even listen to the full audio sample; other times, he deliberated thoughtfully and made a &amp;ldquo;guess&amp;rdquo;.) We hope to show that our model causes longer decision times, suggesting that its generations are more challenging to distinguish from real Bach chorales. Moreover, we hope to show that decision time correlates positively with the chorale score: that is, the higher-scoring a chorale is, the longer it takes a trained listener to determine whether it is real or generated.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a name=&#34;challenges_directions_cite&#34;&gt; Jean-Pierre Briot and François Pachet. &amp;ldquo;Music Generation by Deep Learning – Challenges and Directions.&amp;rdquo;&lt;em&gt;Neural Computing and Applications&lt;/em&gt;, 2018.&lt;/li&gt;
&lt;li&gt;&lt;a name=&#34;deepbach_cite&#34;&gt; Gaetan Hadjeres, Francois Pachet, Frank Nielsen. “DeepBach: a Steerable Model for Bach chorales generation.” &lt;em&gt;The International Conference on Machine Learning&lt;/em&gt; (ICML), 2016.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;The interval types that were excluded are: M6, m6, d1, m-6, d-4, M-6, m7, d4, d-7, M7, d5, m-7, M9, A5, A4, M-7, M-9, A8, P11, M10, P12, A2, d-3, A-4, m10, d7, P15, A-5, and d3.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;(1) first chorale is real. (2) second chorale is real. (3) first chorale is real.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Definition modeling for noun compounds</title>
      <link>https://alisawuffles.github.io/project/noun-compounds/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/noun-compounds/</guid>
      <description>&lt;p&gt;Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”). We identified a large pool of noun compounds in text using a POS-tagging rule, and we are currently incorporating human-authored definitions for these unlabeled noun compounds to evaluate the impact of active learning on generation tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ensemble model for audio source separation</title>
      <link>https://alisawuffles.github.io/project/ensemble/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/ensemble/</guid>
      <description>&lt;p&gt;Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering. We derived a confidence measure based on the clusterability of the embedding space which approximates the separation quality without ground-truth comparison.&lt;/p&gt;

&lt;!-- As humans, we are able to distinguish individual sound sources in complex auditory scenes, as evidenced by our daily encounters with sound. In a crowded coffee shop, we can selectively attend to a friend speaking to us. When we step outdoors, we can identify a passing car, birds chirping, and distant voices. Listening to pop music, we can distinguish the vocals and the instrumental background. As we switch from one audio environment to the next, our brains actually shift among many different grouping mechanisms, such as direction of arrival (grouping sounds from the same spatial location), common fate (grouping sounds that start, stop, and move together), timbral similarity (grouping sources like voices and instruments by timbre), and so on.

In the same way that different audio mixtures require humans to leverage different cues, source separation models are traditionally trained to separate data of a specific domain, and are likely to fail when applied to mixtures unlike the training data.

To see this, consider three models, trained on music mixtures, speech mixtures, and environmental sound mixtures, and an audio clip from each of these three domain. Below, each mixture is separated by each model. We can verify that the separation quality is highest when a mixture is separated by a model trained on the same domain.

[table of audio clips here]

This fundamentally limits how source separation models can be deployed, requiring a user to know about a model&#39;s training to select the correct one for a given mixture. Imagine a hearing aid that automatically switches models when a user moves from the busy cafe to an outdoors environment, in the same way that our brain shifts among auditory cues. That is, we want a general source separation model that can handle mixtures where the source domain is unknown. To do this, we automate selection of the appropriate domain-specific model for a given audio mixture, via a confidence measure that does not require ground truth to estimate separation quality.

## A confidence-based ensemble
We automate selection of the appropriate domain-specific deep clustering source separation model for an audio mixture of unknown domain. We present a confidence measure that does not require ground truth to estimate separation quality, given a model and an audio mixture. We use this confidence measure to automatically select the best model output for the mixture.

### Deep clustering
We apply our method to **deep clustering** source separation networks. In deep clustering, a neural network is trained to map each time-frequency bin in a magnitude spectrogram of an audio mixture to a higher-dimensional embedding, such that bins that primarily contain energy from the same source are near each other and bins whose energy primarily come from different sound sources are far from each other. Given a good mapping, the assignment of bin to source can then be determined by a simple clustering method. All members of the same cluster are assigned to the same source. Because deep clustering performs separation via clustering, we develop a confidence measure that relies on the embedding space, with the core insight that the embedding space produced by deep clustering is indicative of the performance of the algorithm.

![deep-clustering](../../img/deep-clustering.png)

### Confidence measure
Define $X$ as the set of embeddings for every time-frequency point in an audio mixture, where $x\_i$ is the embedding of one point. $X$ is partitioned into $K$ clusters $C\_{k}$, that is, $X = \bigcup_{k=1}^K C_k$. Consider a data point $x_i$ assigned to cluster $C\_k$.

#### Silhouette score
The intercluster distance $a(x\_i)$ captures how much separation exists between the clusters. Specifically, it is the mean distance $x\_i$ and all the points in the nearest cluster $C\_\ell$.
$$a(x\_i) = \frac{1}{|C\_k| - 1} \sum_{\substack{x\_j \in C\_k,\\ x\_i \neq x\_j}} d(x\_i, x\_j)$$

The intracluster distance $b(x\_i)$ captures how dense the clusters are.  Specifically, it is the mean distance (using distance function $d$) between $x\_i$ and all other points in $C\_k$.
$$b(x\_i) = \min\_{\ell \neq k} \frac{1}{|C\_\ell|} \sum\_{x\_j \in C\_\ell} d(x\_i, x\_j)$$

Compute the _silhouette score_ of $x\_i$ as

$$S(x\_i) = \frac{b\left(x\_i\right) - a\left(x\_i\right)}{\max\left(a(x\_i), b(x\_i)\right)}$$

Note $S(x_i)$ ranges from $-1$ to $1$.

#### Posterior strength
For every point $x\_i$ in a dataset $X$, the soft K-means clustering algorithm produces $\gamma\_{ik} \in [0, 1]$, which indicates the membership of the point $x\_i$ in some cluster $C\_k$, also called the \textit{posterior} of the point $x\_i$ in regards to the cluster $C\_k$. The closer that $\gamma\_{ik}$ is to $0$ (not in the cluster) or $1$ (in the cluster), the more sure the assignment of that point. We compute the _posterior strength_ of $x\_i$ as follows:

$$P(x\_i) = \frac{K \left(\max\limits\_{k \in [0, ..., K]} \gamma\_{ij}\right) - 1}{K - 1}$$

The equation maps embeddings that have a maximum posterior of $\frac{1}{K}$ (equal assignment to all clusters) to $0$, and points that have a maximum posterior of $1$ to $1$.

The confidence measure $C(X)$ combines the silhouette score $S(X)$ and posterior strength $P(X)$ through multiplication so that it is high only when both are high. That is, $C(X)=S(X)P(X)$.

Below is a visualization of the confidence measure as applied to the distribution of points in a mixture produced by three trained deep clustering networks, each trained on a different domain. The input is a music mixture. The speech (left) and environmental (right) models return distributions with no clear clusters. The music model (middle) returns a more clusterable distribution, which is reflected by a higher confidence score.

![embedding-visualization](../../../img/embedding-visualization.png)

## Experiments
For each domain that we considered - separating two speakers in a speech mixture, separating vocals from accompaniment in music mixtures, and separating environmental sounds from one another - we train 3 deep clustering networks with identical setups. Each network has 2 BLSTM layers with 300 hidden units each and an embedding size of 20 with sigmoid activation. We trained each network for 80 epochs using the Adam optimizer (learning rate was 2e-4).

### Correlation with SDR
We first demonstrate that the confidence measure correlates well with source to distortion ratio (SDR), a widely used measure of source separation quality. Below, we see a clear relationship between confidence and performance for the speech model as applied to the speech test mixtures. Further, we see that both confidence and performance are a function of the mixture \textit{type}. Same-sex mixtures are harder to separate due to the frequency overlap between the speakers. This is reflected in Figure . For the other domains, we also observe strong correlations. A linear fit between the confidence measure and SDR applied to music mixtures separated by a deep cluster model trained on music mixtures returned an r-value of $0.46$ for vocals and $0.63$ for instrumentals. The linear fit for environmental sounds separated by a model trained on environmental sounds had an r-value of $.70$.

### Performance of the confidence-based ensemble
Then we evaluate the performance of our confidence-based ensemble compared to an oracle ensemble, a random ensemble, and each domain-specific model on general mixtures. The following table shows the performance of various approaches to separating each dataset. Values in the table represent the mean separation quality of each model (based on SDR) when evaluated on all 9,000 test mixtures.

| Approach              | Speech | Music  | Environmental |
|-----------------------|:------:|:------:|:-------------:|
| Ensemble -- oracle     | 8.37   | 6.55   | 12.21         |
| Ensemble -- random     | 4.86   | 4.25   | 2.82          |
| Ensemble -- confidence |**7.61**|**6.47**|**10.52**      |
| Speech model          | 8.29   | 2.06   | 3.03          |
| Music model           | 1.43   | 6.50   | 2.57          |
| Environmental model   | 2.16   | 1.77   | 11.94         |

The top three rows show the performance of three ensemble approaches, which switch between the three domain-specific models via different strategies. The oracle ensemble switches between them with knowledge of the true performance of each model. This is the upper bound for any switching system. The random ensemble randomly selects the model to apply to a given mixture, with equal probability. The confidence ensemble uses our confidence measure to select between the models. For each mixture, all three models are run and confidence measures are computed. The output from the model with the highest confidence is then chosen as the separation. The confidence-based ensemble significantly outperforms the random ensemble. In the case of music mixtures, the confidence-based model achieves almost oracle performance, with mean SDR of $6.47$ compared to $6.55$.

The bottom three rows show the performance of individual domain-specific models on all of the domains we consider. Predictably, every model shows poor performance on domains it was not trained on.

## Conclusion
We have presented a method for effectively combining the output of multiple deep clustering models by switching between them based on mixture domain in an unsupervised fashion. Our method works by analyzing the embedding produced by each deep clustering network to produce a confidence measure that is predictive of separation performance. This confidence measure can be applied to ensembles of any clustering-based separation algorithms. --&gt;
</description>
    </item>
    
    <item>
      <title>Common sense QA dataset</title>
      <link>https://alisawuffles.github.io/project/codah/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/codah/</guid>
      <description>&lt;p&gt;Produced an adversarially generated commonsense question-answer dataset, using a novel question acquisition procedure where workers author questions designed to target weaknesses of state-of-the-art neural QA systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of error types in multi-sense definition generation</title>
      <link>https://alisawuffles.github.io/project/multidef/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/multidef/</guid>
      <description>

&lt;p&gt;Word embeddings are vector representations of words that form the foundation for many NLP tasks. While they have been shown to capture semantic similarities and encode analogical relations between words, these comparison tasks only evaluate an embedding’s information indirectly. In contrast, the task of generating natural language definitions from word embeddings provides a more transparent view of the information captured in a word embedding.&lt;/p&gt;

&lt;p&gt;However, existing definition models are limited in their ability to generate accurate definitions for different senses of the same word. In this paper, we introduce a new method that enables definition modeling for multiple senses. Some selected examples are shown below.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/multidef_ex.png&#34; data-caption=&#34;Selected examples of the multi-sense definition model&amp;rsquo;s outputs&#34;&gt;
&lt;img src=&#34;../../img/multidef_ex.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Selected examples of the multi-sense definition model&amp;rsquo;s outputs
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;quantitative-analysis-of-error-types&#34;&gt;Quantitative analysis of error types&lt;/h2&gt;

&lt;p&gt;In my project, my goal was to better understand the settings under which our model succeeds and fails.&lt;/p&gt;

&lt;p&gt;To evaluate the generated definitions, we manually label outputs as one of four categories: ${\bf I}$ the output is correct; ${\bf II}$ the output has &lt;em&gt;either&lt;/em&gt; a syntax/fluency error or a semantic issue, but not both; ${\bf III}$ the output has both a syntactic and semantic error but is not completely wrong; and ${\bf IV}$ where the output is completely wrong. When evaluating precision and recall, the four labeling categories are given scores $1.0$, $0.6$, $0.3$, and $0.0$ respectively.  A breakdown of the error types is shown below.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/multidef_error_types.png&#34; data-caption=&#34;A breakdown of error types, and the percentage of each in our best model vs. the baseline. For each error type, one example is provided. An output can have more than one error type.&#34;&gt;
&lt;img src=&#34;../../img/multidef_error_types.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A breakdown of error types, and the percentage of each in our best model vs. the baseline. For each error type, one example is provided. An output can have more than one error type.
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;I investigated whether certain attributes of words and atoms are predictive of model performance. The word attributes we considered included the ranking of word frequency, the number of ground-truth definitions of the word, the semantic diversity of ground-truth definitions, and the word embedding norm.  Atom attributes included the atom weight after decomposition and the part of speech of the atom. We used logistic regression with these attributes to predict two different output variables: individual error types and the score from $0$ to $1$. For predicting the score, we trained logistic regression to minimize the cross-entropy between the model output and the score (i.e., we treated the non-0/1 score labels as probabilities). We performed 5-fold validation, where atoms belonging to the same word must always be in the same fold.&lt;/p&gt;

&lt;p&gt;I was unable to predict the individual error labels with accuracy above baseline, which suggests the attributes were not good predictors given the scale of data we had available, and demonstrates that definition generation is still a challenging problem. However, the score prediction model predicts the score with $0.48$ loss, compared to the $0.53$ baseline, using only atom weight as an attribute, which is a significant predictor with p-value $&amp;lt; 0.01$. We hypothesize that this is because atoms with greater weight are more likely to represent more dominant senses that are easier to define.  In fact, the atoms with the top $10\%$ in weight have an average score of $0.35$, substantially higher than the average score of $0.19$ across all atoms.&lt;/p&gt;

&lt;p&gt;In a separate analysis of part of speech, I found that adjectives achieve the highest average score. This is likely the result of having the shortest average output definition length, making the redundancy label (the most common syntactical failure mode) rare. (Adjectives have the redundancy label $3\%$ of the time, compared to $15\%$ for nouns.)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;POS&lt;/th&gt;
&lt;th&gt;average score&lt;/th&gt;
&lt;th&gt;average length&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;adj&lt;/td&gt;
&lt;td&gt;0.39&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;noun&lt;/td&gt;
&lt;td&gt;0.30&lt;/td&gt;
&lt;td&gt;10.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;adv&lt;/td&gt;
&lt;td&gt;0.32&lt;/td&gt;
&lt;td&gt;7.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;verb&lt;/td&gt;
&lt;td&gt;0.31&lt;/td&gt;
&lt;td&gt;5.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This work in definition modeling for single words led me to my next project, where I generate definitions for &lt;a href=&#34;https://alisawuffles.github.io/project/noun-compounds/&#34; target=&#34;_blank&#34;&gt;word compounds&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
