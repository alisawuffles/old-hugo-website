<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alisa Liu</title>
    <link>https://alisawuffles.github.io/</link>
      <atom:link href="https://alisawuffles.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Alisa Liu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 09 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alisawuffles.github.io/img/icon-32.png</url>
      <title>Alisa Liu</title>
      <link>https://alisawuffles.github.io/</link>
    </image>
    
    <item>
      <title>Some study guides I&#39;ve made</title>
      <link>https://alisawuffles.github.io/post/study-guides/</link>
      <pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/post/study-guides/</guid>
      <description>&lt;p&gt;I really enjoy making study guides! Here are a couple I&#39;m especially proud of, and I will update this post as time goes on.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/study_guides/sml.pdf&#34;&gt;EECS 396: Statistical Machine Learning&lt;/a&gt;
The professor shared the topic being tested by every problem on the final, so I organized course content around it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/study_guides/math_312.pdf&#34;&gt;Math 312: Number Theory&lt;/a&gt;
One of my favorite classes (and subjects) of all time. I shared this with the class on Piazza (with permission from &lt;a href=&#34;http://www.cs.uleth.ca/~troupe/&#34;&gt;Dr. Troupe&lt;/a&gt;, of course) and passed it on to many students who would take the course after me. I still hear from people who use it, which makes me really happy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../files/study_guides/math_307.pdf&#34;&gt;Math 307: Applications of Linear Algebra&lt;/a&gt;
More of a formula sheet than a study guide.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Audio captioning</title>
      <link>https://alisawuffles.github.io/project/audio-captioning/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/audio-captioning/</guid>
      <description>&lt;p&gt;Developing a transformer model that generates natural language descriptions of audio clips using the recently released &lt;a href=&#34;https://arxiv.org/abs/1910.09387&#34; target=&#34;_blank&#34;&gt;Clotho dataset&lt;/a&gt;, the first audio caption dataset with captions collected without accompanying video.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmentative generation for Bach chorales</title>
      <link>https://alisawuffles.github.io/project/deepbach/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/deepbach/</guid>
      <description>

&lt;p&gt;Deep learning has become the state-of-the-art approach for music generation, but these networks require massive data. For music generation systems on a specific music domain, such as Bach chorales, Mozart sonatas, or Beatles hits, training data is limited to the real work of musicians during their lifetime. Researchers commonly approach this problem by artificially augmenting the dataset with synthetic data, a technique known as &lt;em&gt;dataset augmentation&lt;/em&gt;. In the musical domain, a natural way is to transpose music examples in all keys, or to segment examples into many shorter fragments. However, these all suffer musical limitations: the original key signature is deliberately chosen by the composer and typically associated with certain emotions and expressive qualities, and segmenting examples prevents a network from being able to learn the long-term coherence of human-written music.&lt;/p&gt;

&lt;p&gt;We propose an alternative method for data augmentation, which we call &lt;em&gt;augmentative generation&lt;/em&gt;. After training a base model on the original dataset, we generate some output from the model, filter it by a hand-crafted score function, and feed it back to the model as new training data to update the model. This generate-update iteration can be continued until it stops improving the model. We try this method with chorale generation in the style of J.S. Bach, using the &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;DeepBach&lt;/a&gt; model. For Bach chorale generation, gold data is inherently limited to the 389 chorales Bach wrote in his lifetime. While this is considered a large output for a single form for a composer, 389 training examples are often not enough in training machine learning models, motivating our proposed solution.&lt;/p&gt;

&lt;p&gt;While augmentative generation can be applied to any generation system where training data is limited, the score function must be handcrafted for each domain. We argue that this is a good thing, since a limitation of deep learning techniques compared to handcrafted models is that they do not offer direct ways of controlling generation. But given that we know things about what some widely accepted music should sound like, we believe that a handcrafted function allows musicians to incorporate important domain knowledge into the music generation task, while still using the representational capacity of a neural network. That is, rather than enforcing things about our output, we can enforce things about the input that our model is trained on.&lt;/p&gt;

&lt;h1 id=&#34;proposed-method&#34;&gt;Proposed method&lt;/h1&gt;

&lt;p&gt;First we train the base model on the original 351 Bach chorales. Compared to the original DeepBach paper, we do not use any transpositions of Bach chorales.&lt;/p&gt;

&lt;p&gt;Then, we perform a series of generation-train updates. In each iteration, we generate $N$ chorales and score each one. We set our threshold as the score of the lowest-scoring Bach chorale, with the justification that every real Bach chorale should be selected by our score function. We update the model by training on the selected examples for $M$ epochs.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/architecture.png&#34; data-caption=&#34;Augmentative generation training procedure applied to DeepBach&#34;&gt;
&lt;img src=&#34;../../img/architecture.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Augmentative generation training procedure applied to DeepBach
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;deepbach&#34;&gt;DeepBach&lt;/h2&gt;

&lt;p&gt;We apply our method to &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;DeepBach&lt;/a&gt;, an architecture that was designed for the generation of Bach chorales. It combines two recurrent networks (LSTMs) and two feedforward networks. One recurrent network sums up past information, one recurrent network sums up future information, and a non-recurrent network represents notes occurring at the same time. The outputs of the three models are merged and passed as the input of a final feedforward neural network, whose output activation function is softmax. This architecture is replicated four times, one for each voice in a chorale. Generation is done by sampling, using a pseudo-Gibbs sampling incremental and iterative algorithm.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/deepbach_architecture.png&#34; data-caption=&#34;DeepBach’s neural network architecture for prediction of the soprano voice&#34;&gt;
&lt;img src=&#34;../../img/deepbach_architecture.png&#34; alt=&#34;&#34; width=&#34;30%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    DeepBach’s neural network architecture for prediction of the soprano voice
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We use the same data representation as in the original DeepBach paper. In particular, notes are encoded as MIDI pitches and discretized into sixteenth notes. A hold symbol “&amp;ndash;” is used to indicate whether the preceding note is held. The fermata symbol for Bach chorales is explicitly considered in the metadata to help produce structure and coherent phrases.&lt;/p&gt;

&lt;h2 id=&#34;score-function&#34;&gt;Score function&lt;/h2&gt;

&lt;p&gt;The score function determines which generated chorales are used to update the model. Given a chorale, the score function should output a value that captures how much it looks like real Bach chorales. Our basic method is to compare feature distributions between a given chorale and real Bach chorales. (To be clear, a distribution is a histogram of counts, normalized by the total number of notes.) For each feature $f$, we use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wasserstein_metric&#34; target=&#34;_blank&#34;&gt;Wasserstein metric&lt;/a&gt; to measure the distance between the distribution $P_{c}^f$ of the given chorale and the ground-truth distribution $P_{\text{Bach}}^f$ over real Bach chorales. The resulting value, $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$, is weighted by $w^f$. The overall score is therefore a weighted sum of these distances over all features $f$. That is,
$$S( c )=\sum_{f\in\text{features}} Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)\cdot w^f$$
Since low distances represent high similarity between a given chorale and real Bach chorales, we subtract $S( c )$ from a large constant so that higher scores correspond to better chorales.&lt;/p&gt;

&lt;p&gt;The weights $w^f$ are carefully handpicked in an iterative process where we modify the score, judge its correlation with our own perception of chorale quality, and modify the score further. We take into account the natural value range of $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$ and the importance of the feature to a good chorale.&lt;/p&gt;

&lt;p&gt;The features we use in the score function are notes, rhythm, interval, parallel errors, and other voice leading errors. We describe these next.&lt;/p&gt;

&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;The note distribution is the distribution of notes in scale degrees. The ground-truth note distribution for Bach chorales in major and Bach chorales in minor are calculated separately. Below are the ground-truth distributions.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/note_distributions.png&#34; data-caption=&#34;Distribution of notes (in scale degrees) over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/note_distributions.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of notes (in scale degrees) over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For an example, consider the following generated chorale and its note distribution.&lt;/p&gt;

&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex.png&#34; data-caption=&#34;The score of a chorale generated by our base model&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex.png&#34; alt=&#34;&#34; width=&#34;70%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score of a chorale generated by our base model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex_distribution.png&#34; data-caption=&#34;The note distribution (in scale degrees) for an example chorale&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The note distribution (in scale degrees) for an example chorale
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Since this chorale is in G major, the chorale note distribution is compared to the ground-truth major note distribution to obtain $Wass\left(P_{c}^\text{note},P_{\text{Bach}}^{\text{note},M}\right)$.&lt;/p&gt;

&lt;h3 id=&#34;rhythm&#34;&gt;Rhythm&lt;/h3&gt;

&lt;p&gt;The rhythm distribution is the distribution of note lengths in units of quarter notes.&lt;/p&gt;

&lt;h3 id=&#34;interval&#34;&gt;Interval&lt;/h3&gt;

&lt;p&gt;The interval distribution is the distribution of interval sizes.&lt;/p&gt;

&lt;h3 id=&#34;parallel-errors&#34;&gt;Parallel errors&lt;/h3&gt;

&lt;p&gt;The parallel errors distribution is the distribution of parallel 5th and parallel octaves errors. However, note here that what matters is not only the ratio of parallel 5ths and parallel octaves, but the count of these errors relative to the total number of notes. Therefore, $w^\text{parallel}$ includes a term
$$\frac{\text{error to note ratio of chorale }c}{\text{error to note ratio of Bach}}$$&lt;/p&gt;

&lt;h3 id=&#34;other-errors&#34;&gt;Other errors&lt;/h3&gt;

&lt;p&gt;The other errors distribution is the distribution of hidden 5ths, hidden octaves, voice overlaps, voice crossings, and voice range violations. Similarly to the previous feature, $w^\text{errors}$ includes a term to account for the absolute count of errors, in addition to the relative counts in the distribution.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;In our experiments, we first show that the score function effectively distinguishes real Bach chorales from generations. Then we carry out human evaluations to compare generations of three models: our model trained through augmentative generation on Bach chorales and high-quality generations, our base model trained on only Bach chorales, and the model in the original DeepBach paper trained on Bach chorales and their transpositions.&lt;/p&gt;

&lt;h2 id=&#34;evaluating-the-score-function&#34;&gt;Evaluating the score function&lt;/h2&gt;

&lt;p&gt;Because the score function measures how similar a given chorale is to real Bach chorales, a simple evaluation method is to ensure that real Bach chorales score higher than generated chorales. To see this, we plot the distribution of scores for real Bach chorales and chorales generated by our base model.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/score_distribution.png&#34; data-caption=&#34;The score distribution for Bach chorales and generated chorales&#34;&gt;
&lt;img src=&#34;../../img/score_distribution.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score distribution for Bach chorales and generated chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;

These distributions show that Bach chorales score much better on average than generated chorales, which is an indication that the score function serves its purpose of measuring similarity to real Bach chorales. We also see that generated chorales display high variation in quality, which demonstrates the importance of a employing a filter on the generated chorales.&lt;/p&gt;

&lt;p&gt;Note that generated chorales which overlap with real chorales in score are the ones that would be selected for training on.&lt;/p&gt;

&lt;h2 id=&#34;model-details&#34;&gt;Model details&lt;/h2&gt;

&lt;p&gt;We updated the model for $43$ iterations. Each iteration, we generated $50$ training examples. Of note, after this process our model saw less than one-fifth of the training data used in the original DeepBach paper, which consists of Bach chorales and their transpositions.&lt;/p&gt;

&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;In the paired discrimination task, participants with music training are presented with pairs of audio clips where one is a real Bach chorale and one is generated output. The generated output comes from one of three models: our model trained through augmentative generation on Bach chorales and high-quality generations, our base model trained on only Bach chorales, and the model in the original DeepBach paper trained on Bach chorales and their transpositions. Participants are told that not all generated outputs are produced by the same model. We then compare human accuracy at detecting generated music for each model.&lt;/p&gt;

&lt;p&gt;For preliminary experiments, we have $n=1$ participant with a conservatory degree in piano performance. We presented 30 pairs of audio samples, using $30$ Bach chorales and $10$ randomly selected generations from each model.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + transpositions)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + augmentative generation)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Unfortunately our experiments were conducted under the grueling demands of finals week, and not yet extensive enough to be discriminative. Future work is discussed below.&lt;/p&gt;

&lt;h1 id=&#34;music-samples&#34;&gt;Music samples&lt;/h1&gt;

&lt;p&gt;See if you can discriminate between Bach chorales and generated chorales yourself! In each of the following pairs, one chorale is composed by Bach and one is generated. The answers are behind the references.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales), which is our base model.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;4_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;4_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + transpositions), from the original DeepBach paper.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;3_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;3_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + augmentative generation), which is our contribution.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;6_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;6_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;upcoming-work-and-ideas&#34;&gt;Upcoming work and ideas&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Every handful of generate-train updates, do another pass through the original Bach chorale training data. This way, we ensure that the model does not &amp;ldquo;forget&amp;rdquo; what gold data looks like.&lt;/li&gt;
&lt;li&gt;Show (hopefully) that the score distribution of generated chorales is increasing over time and becoming more similar to the score distribution over Bach chorales. This shows that according to the score function, generations are improving as generate-train updates go on.&lt;/li&gt;
&lt;li&gt;Currently, there is no method of determining when to discontinue generate-train iterations. We need to implement code to evaluate the validation loss at the end of every iteration, in order to stop when training on more generations is no longer improving model performance on validation data.&lt;/li&gt;
&lt;li&gt;Conduct a much more extensive version of the human evaluation. In addition to measuring discrimination accuracy, we intend to measure decision time as an indicator of confidence. This is motivated by an observation we made in our preliminary experiment: even though our participant displayed the same performance for each model, his amount hesitation varied significantly (sometimes, he did not even listen to the full audio sample; other times, he deliberated thoughtfully and made a &amp;ldquo;guess&amp;rdquo;.) We hope to show that our model causes longer decision times, suggesting that its generations are more challenging to distinguish from real Bach chorales. Moreover, we hope to show that decision time correlates positively with the chorale score: that is, the higher-scoring a chorale is, the longer it takes a trained listener to determine whether it is real or generated.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Jean-Pierre Briot and François Pachet. &amp;ldquo;Music Generation by Deep Learning – Challenges and Directions.&amp;rdquo;&lt;em&gt;Neural Computing and Applications&lt;/em&gt;, 2018. &lt;a name=&#34;challenges_directions_cite&#34;&gt;&lt;/li&gt;
&lt;li&gt;Gaetan Hadjeres, Francois Pachet, Frank Nielsen. “DeepBach: a Steerable Model for Bach chorales generation.” &lt;em&gt;The International Conference on Machine Learning&lt;/em&gt; (ICML), 2016. &lt;a name=&#34;deepbach_cite&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answers: 1. first chorale is real. 2. second chorale is real. 3. first chorale is real.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Definition modeling for noun compounds</title>
      <link>https://alisawuffles.github.io/project/noun-compounds/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/noun-compounds/</guid>
      <description>&lt;p&gt;Developing a neural language model that generates definitions and paraphrases of noun compounds (e.g. “caramel popcorn”). We identified a large pool of noun compounds in text using a POS-tagging rule, and we are currently incorporating human-authored definitions for these unlabeled noun compounds to evaluate the impact of active learning on generation tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ensemble model for audio source separation</title>
      <link>https://alisawuffles.github.io/project/ensemble/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/ensemble/</guid>
      <description>&lt;p&gt;Built an ensemble model for audio source separation that can handle mixtures whose source domain is unknown, using a confidence measure to mediate among domain-specific models based on deep clustering. We derived a confidence measure based on the clusterability of the embedding space which approximates the separation quality without ground-truth comparison.&lt;/p&gt;

&lt;!-- As humans, we are able to distinguish individual sound sources in complex auditory scenes, as evidenced by our daily encounters with sound. In a crowded coffee shop, we can selectively attend to a friend speaking to us. When we step outdoors, we can identify a passing car, birds chirping, and distant voices. Listening to pop music, we can distinguish the vocals and the instrumental background. As we switch from one audio environment to the next, our brains actually shift among many different grouping mechanisms, such as direction of arrival (grouping sounds from the same spatial location), common fate (grouping sounds that start, stop, and move together), timbral similarity (grouping sources like voices and instruments by timbre), and so on.

In the same way that different audio mixtures require humans to leverage different cues, source separation models are traditionally trained to separate data of a specific domain, and are likely to fail when applied to mixtures unlike the training data.

To see this, consider three models, trained on music mixtures, speech mixtures, and environmental sound mixtures, and an audio clip from each of these three domain. Below, each mixture is separated by each model. We can verify that the separation quality is highest when a mixture is separated by a model trained on the same domain.

[table of audio clips here]

This fundamentally limits how source separation models can be deployed, requiring a user to know about a model&#39;s training to select the correct one for a given mixture. Imagine a hearing aid that automatically switches models when a user moves from the busy cafe to an outdoors environment, in the same way that our brain shifts among auditory cues. That is, we want a general source separation model that can handle mixtures where the source domain is unknown. To do this, we automate selection of the appropriate domain-specific model for a given audio mixture, via a confidence measure that does not require ground truth to estimate separation quality.

## A confidence-based ensemble
We automate selection of the appropriate domain-specific deep clustering source separation model for an audio mixture of unknown domain. We present a confidence measure that does not require ground truth to estimate separation quality, given a model and an audio mixture. We use this confidence measure to automatically select the best model output for the mixture.

### Deep clustering
We apply our method to **deep clustering** source separation networks. In deep clustering, a neural network is trained to map each time-frequency bin in a magnitude spectrogram of an audio mixture to a higher-dimensional embedding, such that bins that primarily contain energy from the same source are near each other and bins whose energy primarily come from different sound sources are far from each other. Given a good mapping, the assignment of bin to source can then be determined by a simple clustering method. All members of the same cluster are assigned to the same source. Because deep clustering performs separation via clustering, we develop a confidence measure that relies on the embedding space, with the core insight that the embedding space produced by deep clustering is indicative of the performance of the algorithm.

![deep-clustering](../../img/deep-clustering.png)

### Confidence measure
Define $X$ as the set of embeddings for every time-frequency point in an audio mixture, where $x\_i$ is the embedding of one point. $X$ is partitioned into $K$ clusters $C\_{k}$, that is, $X = \bigcup_{k=1}^K C_k$. Consider a data point $x_i$ assigned to cluster $C\_k$.

#### Silhouette score
The intercluster distance $a(x\_i)$ captures how much separation exists between the clusters. Specifically, it is the mean distance $x\_i$ and all the points in the nearest cluster $C\_\ell$.
$$a(x\_i) = \frac{1}{|C\_k| - 1} \sum_{\substack{x\_j \in C\_k,\\ x\_i \neq x\_j}} d(x\_i, x\_j)$$

The intracluster distance $b(x\_i)$ captures how dense the clusters are.  Specifically, it is the mean distance (using distance function $d$) between $x\_i$ and all other points in $C\_k$.
$$b(x\_i) = \min\_{\ell \neq k} \frac{1}{|C\_\ell|} \sum\_{x\_j \in C\_\ell} d(x\_i, x\_j)$$

Compute the _silhouette score_ of $x\_i$ as

$$S(x\_i) = \frac{b\left(x\_i\right) - a\left(x\_i\right)}{\max\left(a(x\_i), b(x\_i)\right)}$$

Note $S(x_i)$ ranges from $-1$ to $1$.

#### Posterior strength
For every point $x\_i$ in a dataset $X$, the soft K-means clustering algorithm produces $\gamma\_{ik} \in [0, 1]$, which indicates the membership of the point $x\_i$ in some cluster $C\_k$, also called the \textit{posterior} of the point $x\_i$ in regards to the cluster $C\_k$. The closer that $\gamma\_{ik}$ is to $0$ (not in the cluster) or $1$ (in the cluster), the more sure the assignment of that point. We compute the _posterior strength_ of $x\_i$ as follows:

$$P(x\_i) = \frac{K \left(\max\limits\_{k \in [0, ..., K]} \gamma\_{ij}\right) - 1}{K - 1}$$

The equation maps embeddings that have a maximum posterior of $\frac{1}{K}$ (equal assignment to all clusters) to $0$, and points that have a maximum posterior of $1$ to $1$.

The confidence measure $C(X)$ combines the silhouette score $S(X)$ and posterior strength $P(X)$ through multiplication so that it is high only when both are high. That is, $C(X)=S(X)P(X)$.

Below is a visualization of the confidence measure as applied to the distribution of points in a mixture produced by three trained deep clustering networks, each trained on a different domain. The input is a music mixture. The speech (left) and environmental (right) models return distributions with no clear clusters. The music model (middle) returns a more clusterable distribution, which is reflected by a higher confidence score.

![embedding-visualization](../../../img/embedding-visualization.png)

## Experiments
For each domain that we considered - separating two speakers in a speech mixture, separating vocals from accompaniment in music mixtures, and separating environmental sounds from one another - we train 3 deep clustering networks with identical setups. Each network has 2 BLSTM layers with 300 hidden units each and an embedding size of 20 with sigmoid activation. We trained each network for 80 epochs using the Adam optimizer (learning rate was 2e-4).

### Correlation with SDR
We first demonstrate that the confidence measure correlates well with source to distortion ratio (SDR), a widely used measure of source separation quality. Below, we see a clear relationship between confidence and performance for the speech model as applied to the speech test mixtures. Further, we see that both confidence and performance are a function of the mixture \textit{type}. Same-sex mixtures are harder to separate due to the frequency overlap between the speakers. This is reflected in Figure . For the other domains, we also observe strong correlations. A linear fit between the confidence measure and SDR applied to music mixtures separated by a deep cluster model trained on music mixtures returned an r-value of $0.46$ for vocals and $0.63$ for instrumentals. The linear fit for environmental sounds separated by a model trained on environmental sounds had an r-value of $.70$.

### Performance of the confidence-based ensemble
Then we evaluate the performance of our confidence-based ensemble compared to an oracle ensemble, a random ensemble, and each domain-specific model on general mixtures. The following table shows the performance of various approaches to separating each dataset. Values in the table represent the mean separation quality of each model (based on SDR) when evaluated on all 9,000 test mixtures.

| Approach              | Speech | Music  | Environmental |
|-----------------------|:------:|:------:|:-------------:|
| Ensemble -- oracle     | 8.37   | 6.55   | 12.21         |
| Ensemble -- random     | 4.86   | 4.25   | 2.82          |
| Ensemble -- confidence |**7.61**|**6.47**|**10.52**      |
| Speech model          | 8.29   | 2.06   | 3.03          |
| Music model           | 1.43   | 6.50   | 2.57          |
| Environmental model   | 2.16   | 1.77   | 11.94         |

The top three rows show the performance of three ensemble approaches, which switch between the three domain-specific models via different strategies. The oracle ensemble switches between them with knowledge of the true performance of each model. This is the upper bound for any switching system. The random ensemble randomly selects the model to apply to a given mixture, with equal probability. The confidence ensemble uses our confidence measure to select between the models. For each mixture, all three models are run and confidence measures are computed. The output from the model with the highest confidence is then chosen as the separation. The confidence-based ensemble significantly outperforms the random ensemble. In the case of music mixtures, the confidence-based model achieves almost oracle performance, with mean SDR of $6.47$ compared to $6.55$.

The bottom three rows show the performance of individual domain-specific models on all of the domains we consider. Predictably, every model shows poor performance on domains it was not trained on.

## Conclusion
We have presented a method for effectively combining the output of multiple deep clustering models by switching between them based on mixture domain in an unsupervised fashion. Our method works by analyzing the embedding produced by each deep clustering network to produce a confidence measure that is predictive of separation performance. This confidence measure can be applied to ensembles of any clustering-based separation algorithms. --&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection for Deep Audio Source Separation via Clustering Analysis</title>
      <link>https://alisawuffles.github.io/publication/ensemble/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-sense Definition Modeling using Word Sense Decompositions</title>
      <link>https://alisawuffles.github.io/publication/multidef/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/multidef/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense</title>
      <link>https://alisawuffles.github.io/publication/codah/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/codah/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Common sense QA dataset</title>
      <link>https://alisawuffles.github.io/project/codah/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/codah/</guid>
      <description>&lt;p&gt;Produced an adversarially generated commonsense question-answer dataset, using a novel question acquisition procedure where workers author questions designed to target weaknesses of state-of-the-art neural QA systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of error types in multi-sense definition generation</title>
      <link>https://alisawuffles.github.io/project/multidef/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/multidef/</guid>
      <description>&lt;p&gt;Evaluated the settings under which a multi-sense definition modeling system succeeded and failed by investigating whether certain attributes of words and atoms were predictive of model performance. Used logistic regression with these attributes (e.g. word frequency, embedding norm, part of speech) to predict individual error types as well as a manual evaluation score. We found that atom weight was a significant predictor for score, showing that atoms with greater weights are likely to represent more dominant sense that are easier to define.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparison of Discourse Surrounding CRISPR/Cas9 in the Media and Peer-Reviewed Literature</title>
      <link>https://alisawuffles.github.io/publication/crispr/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/crispr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
