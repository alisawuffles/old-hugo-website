<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alisa Liu</title>
    <link>https://alisawuffles.github.io/authors/admin/</link>
      <atom:link href="https://alisawuffles.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Alisa Liu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 01 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://alisawuffles.github.io/img/icon-32.png</url>
      <title>Alisa Liu</title>
      <link>https://alisawuffles.github.io/authors/admin/</link>
    </image>
    
    <item>
      <title>Augmentative generation for Bach chorales</title>
      <link>https://alisawuffles.github.io/project/deepbach/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/project/deepbach/</guid>
      <description>

&lt;p&gt;Deep learning has become the state-of-the-art approach for music generation, but these networks require massive data. For music generation systems on a specific music domain, such as Bach chorales, Mozart sonatas, or Beatles hits, training data is limited to the real work of musicians during their lifetime. Researchers commonly approach this problem by artificially augmenting the dataset with synthetic data, a technique known as &lt;em&gt;dataset augmentation&lt;/em&gt;. In the musical domain, a natural way is to transpose music examples in all keys, or to segment examples into many shorter fragments. However, these all suffer musical limitations: the original key signature is deliberately chosen by the composer and typically associated with certain emotions and expressive qualities, and segmenting examples prevents a network from being able to learn the long-term coherence of human-written music.&lt;/p&gt;

&lt;p&gt;We propose an alternative method for data augmentation, which we call &lt;em&gt;augmentative generation&lt;/em&gt;. After training a base model on the original dataset, we generate some output from the model, filter it by a hand-crafted score function, and feed it back to the model as new training data to update the model. These generate-update iterations can be continued until it stops improving the model. We try this method with chorale generation in the style of J.S. Bach, using the &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;DeepBach&lt;/a&gt; model. For Bach chorale generation, gold data is inherently limited to the 389 chorales Bach wrote in his lifetime. While this is considered a large output for a single form for a composer, 389 training examples are often not enough in training machine learning models, motivating our proposed solution.&lt;/p&gt;

&lt;p&gt;While augmentative generation can be applied to any generation system where training data is limited, the score function must be handcrafted for each domain. We argue that this is a good thing, since a limitation of deep learning techniques compared to handcrafted models is that they do not offer direct ways of controlling generation. But given that we know some things about what widely accepted music (like Bach chorales or Mozart sonatas) should sound like, we believe that a handcrafted function allows musicians to incorporate important domain knowledge into the music generation task, while still using the representational capacity of a neural network. That is, rather than enforcing things about our output, we can enforce things about the input that our model is trained on.&lt;/p&gt;

&lt;h1 id=&#34;proposed-method&#34;&gt;Proposed method&lt;/h1&gt;

&lt;p&gt;First we train the base model on the original 351 Bach chorales. Compared to the original DeepBach paper, we do not use any transpositions of Bach chorales.&lt;/p&gt;

&lt;p&gt;Then, we perform a series of generation-train updates. In each iteration, we generate $N$ chorales and score each one. We set our threshold as the score of the lowest-scoring Bach chorale, with the intuition that every real Bach chorale should be selected by our score function. We update the model by training on the selected examples for $M$ epochs.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/architecture.png&#34; data-caption=&#34;Augmentative generation training procedure applied to DeepBach&#34;&gt;
&lt;img src=&#34;../../img/architecture.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Augmentative generation training procedure applied to DeepBach
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;deepbach&#34;&gt;DeepBach&lt;/h2&gt;

&lt;p&gt;We apply our method to &lt;a href=&#34;#deepbach_cite&#34; target=&#34;_blank&#34;&gt;DeepBach&lt;/a&gt;, an architecture that was designed for the generation of Bach chorales. It combines two recurrent networks (LSTMs) and two feedforward networks. One recurrent network sums up past information, one recurrent network sums up future information, and a non-recurrent network represents notes occurring at the same time. The outputs of the three models are merged and passed as the input of a final feedforward neural network, whose output activation function is softmax. This architecture is replicated four times, one for each voice in a chorale. Generation is done by sampling, using a pseudo-Gibbs sampling incremental and iterative algorithm.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/deepbach_architecture.png&#34; data-caption=&#34;DeepBach’s neural network architecture for prediction of the soprano voice&#34;&gt;
&lt;img src=&#34;../../img/deepbach_architecture.png&#34; alt=&#34;&#34; width=&#34;30%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    DeepBach’s neural network architecture for prediction of the soprano voice
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We use the same data representation as in the original DeepBach paper. In particular, notes are encoded as MIDI pitches and discretized into sixteenth notes. A hold symbol “&amp;ndash;” is used to indicate whether the preceding note is held. The fermata symbol for Bach chorales is explicitly considered in the metadata to help produce structure and coherent phrases.&lt;/p&gt;

&lt;h2 id=&#34;score-function&#34;&gt;Score function&lt;/h2&gt;

&lt;p&gt;The score function determines which generated chorales are used to update the model. Given a chorale, the score function should output a value that captures how much it looks like real Bach chorales. Our basic method is to compare feature distributions between a given chorale and real Bach chorales. (To be clear, a distribution is a histogram of counts, normalized by the total number of notes.) For each feature $f$, we use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wasserstein_metric&#34; target=&#34;_blank&#34;&gt;Wasserstein metric&lt;/a&gt; to measure the distance between the distribution $P_{c}^f$ of the given chorale $c$ and the ground-truth distribution $P_{\text{Bach}}^f$ over real Bach chorales. The resulting value, $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$, is weighted by $w^f$. The overall score is therefore a weighted sum of these distances over all features $f$. That is,
$$S( c )=\sum_{f\in\text{features}} Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)\cdot w^f$$
Since low distances represent high similarity between a given chorale and real Bach chorales, we subtract $S( c )$ from a large constant so that higher scores correspond to better chorales.&lt;/p&gt;

&lt;p&gt;The weights $w^f$ are carefully handpicked in an iterative process where we modify the weights, judge its correlation with our own perception of chorale quality, and modify the weights further. For each feature we take into account the natural value range of $Wass\left(P_{c}^f,P_{\text{Bach}}^f\right)$, its empirical usefulness in distinguishing chorale quality, and its overall musical importance to a good chorale.&lt;/p&gt;

&lt;p&gt;The features we use in the score function are notes, rhythm, interval, parallel errors, and other voice leading errors. We describe these next.&lt;/p&gt;

&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;

&lt;p&gt;The note distribution is the distribution of notes in scale degrees. The ground-truth note distribution for Bach chorales in major and Bach chorales in minor are calculated separately. Below are the ground-truth distributions.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/note_distributions.png&#34; data-caption=&#34;Distribution of notes (in scale degrees) over Bach chorales&#34;&gt;
&lt;img src=&#34;../../img/note_distributions.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of notes (in scale degrees) over Bach chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;For an example, consider the following generated chorale and its note distribution.&lt;/p&gt;

&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex.png&#34; data-caption=&#34;The score of a chorale generated by our base model&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex.png&#34; alt=&#34;&#34; width=&#34;70%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score of a chorale generated by our base model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/chorale_ex_distribution.png&#34; data-caption=&#34;The note distribution (in scale degrees) for an example chorale&#34;&gt;
&lt;img src=&#34;../../img/chorale_ex_distribution.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The note distribution (in scale degrees) for an example chorale
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Since this chorale is in G major, the chorale note distribution is compared to the ground-truth major note distribution to obtain $Wass\left(P_{c}^\text{note},P_{\text{Bach}}^{\text{note},M}\right)$.&lt;/p&gt;

&lt;h3 id=&#34;rhythm&#34;&gt;Rhythm&lt;/h3&gt;

&lt;p&gt;The rhythm distribution is the distribution of note lengths in units of quarter notes.&lt;/p&gt;

&lt;h3 id=&#34;interval&#34;&gt;Interval&lt;/h3&gt;

&lt;p&gt;The interval distribution is the distribution of interval sizes.&lt;/p&gt;

&lt;h3 id=&#34;parallel-errors&#34;&gt;Parallel errors&lt;/h3&gt;

&lt;p&gt;The parallel errors distribution is the distribution of parallel 5th and parallel octaves errors. However, note here that what matters is not only the ratio of parallel 5ths and parallel octaves, but the count of these errors relative to the total number of notes. Therefore, $w^\text{parallel}$ includes a term
$$\frac{\text{error to note ratio of chorale }c}{\text{error to note ratio of Bach}}$$&lt;/p&gt;

&lt;h3 id=&#34;other-errors&#34;&gt;Other errors&lt;/h3&gt;

&lt;p&gt;The other errors distribution is the distribution of hidden 5ths, hidden octaves, voice overlaps, voice crossings, and voice range violations. Similarly to the previous feature, $w^\text{errors}$ includes a term to account for the absolute count of errors, in addition to the relative counts in the distribution.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;In our experiments, we first show that the score function effectively distinguishes real Bach chorales from generations. Then we carry out human evaluations to compare generations of three models: our model trained through augmentative generation, our base model, and the model in the original DeepBach paper.&lt;/p&gt;

&lt;h2 id=&#34;evaluating-the-score-function&#34;&gt;Evaluating the score function&lt;/h2&gt;

&lt;p&gt;Because the score function measures how similar a given chorale is to real Bach chorales, a simple evaluation method is to validate that real Bach chorales score higher than generated chorales. Therefore, we plot the distribution of scores for real Bach chorales and chorales generated by our base model.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;../../img/score_distribution.png&#34; data-caption=&#34;The score distribution for Bach chorales and generated chorales&#34;&gt;
&lt;img src=&#34;../../img/score_distribution.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The score distribution for Bach chorales and generated chorales
  &lt;/figcaption&gt;


&lt;/figure&gt;

These plots show that Bach chorales score much better on average than generated chorales, indicating that the score function serves its purpose of measuring similarity to real Bach chorales. We also see that generated chorales display high variation in quality, which demonstrates the importance of an external evaluation metric for generated chorales.&lt;/p&gt;

&lt;p&gt;Note that based on our score threshold, the generated chorales which which would be selected can be visualized as those that &amp;ldquo;overlap&amp;rdquo; with real chorales in the score distribution.&lt;/p&gt;

&lt;h2 id=&#34;model-details&#34;&gt;Model details&lt;/h2&gt;

&lt;p&gt;We updated the model for $43$ iterations. Each iteration, we generated $50$ training examples for consideration. Of note, after this process our model saw less than one-fifth of the training data used in the original DeepBach paper, which consists of Bach chorales and their transpositions.&lt;/p&gt;

&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;In the paired discrimination task, participants with music training are presented with pairs of audio clips where one is a real Bach chorale and one is generated output. The generated output comes from one of three models: our model trained through augmentative generation on Bach chorales and high-quality generations, our base model trained on only Bach chorales, and the model in the original DeepBach paper trained on Bach chorales and their transpositions. Participants are told that not all generated outputs are produced by the same model. We then compare human accuracy at detecting generated music for each model.&lt;/p&gt;

&lt;p&gt;For preliminary experiments, we have $n=1$ participant with a conservatory degree in piano performance. We presented 30 pairs of audio samples, using $30$ Bach chorales and $10$ randomly selected generations from each model. Each audio sample represents $4$ measures of music, rendered with MuseScore at $80$ beats per minute.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + transpositions)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;DeepBach (trained on Bach chorales + augmentative generation)&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Unfortunately our experiments were conducted under the grueling demands of finals week, and not yet extensive enough to be discriminative. Future work is discussed below.&lt;/p&gt;

&lt;h1 id=&#34;music-samples&#34;&gt;Music samples&lt;/h1&gt;

&lt;p&gt;See if you can discriminate between Bach chorales and generated chorales yourself! In each of the following pairs, one chorale is composed by Bach and one is generated. The answers are behind the references.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales), which is our base model.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/4_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/4_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + transpositions), from the original DeepBach paper.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/3_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/3_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generated chorale comes from DeepBach (trained on Bach chorales + augmentative generation), which is our contribution.
&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/6_real.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;

&lt;audio controls style=&#34;width: 100%; margin-bottom: 20px&#34;&gt;
    
    
    &lt;source src=&#34;../../audio/6_fake.wav&#34; type=&#34;audio/wav&#34;&gt;
    Your browser does not support the audio element.
&lt;/audio&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;upcoming-work-and-ideas&#34;&gt;Upcoming work and ideas&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Every handful of generate-train updates, do another pass through the original Bach chorale training data. This way, we ensure that the model does not &amp;ldquo;forget&amp;rdquo; what gold data looks like.&lt;/li&gt;
&lt;li&gt;Show (hopefully) that the score distribution of generated chorales is increasing over time and becoming more similar to the score distribution over Bach chorales. This shows that according to the score function, generations are improving as generate-train updates go on.&lt;/li&gt;
&lt;li&gt;Currently, there is no method of determining when to discontinue generate-train iterations. We need to implement code to evaluate the validation loss at the end of every iteration, in order to stop when training on more generations is no longer improving model performance on validation data.&lt;/li&gt;
&lt;li&gt;Conduct a much more extensive version of the human evaluation. In addition to measuring discrimination accuracy, we intend to measure decision time as an indicator of confidence. This is motivated by an observation we made in our preliminary experiment: even though our participant displayed the same performance for each model, his amount hesitation varied significantly (sometimes, he did not even listen to the full audio sample; other times, he deliberated thoughtfully and made a &amp;ldquo;guess&amp;rdquo;.) We hope to show that our model causes longer decision times, suggesting that its generations are more challenging to distinguish from real Bach chorales. Moreover, we hope to show that decision time correlates positively with the chorale score: that is, the higher-scoring a chorale is, the longer it takes a trained listener to determine whether it is real or generated.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Jean-Pierre Briot and François Pachet. &amp;ldquo;Music Generation by Deep Learning – Challenges and Directions.&amp;rdquo;&lt;em&gt;Neural Computing and Applications&lt;/em&gt;, 2018. &lt;a name=&#34;challenges_directions_cite&#34;&gt;&lt;/li&gt;
&lt;li&gt;Gaetan Hadjeres, Francois Pachet, Frank Nielsen. “DeepBach: a Steerable Model for Bach chorales generation.” &lt;em&gt;The International Conference on Machine Learning&lt;/em&gt; (ICML), 2016. &lt;a name=&#34;deepbach_cite&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Answers: 1. first chorale is real. 2. second chorale is real. 3. first chorale is real.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection for Deep Audio Source Separation via Clustering Analysis</title>
      <link>https://alisawuffles.github.io/publication/ensemble/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-sense Definition Modeling using Word Sense Decompositions</title>
      <link>https://alisawuffles.github.io/publication/multidef/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/multidef/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CODAH: An Adversarially Authored Question-Answer Dataset for Common Sense</title>
      <link>https://alisawuffles.github.io/publication/codah/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/codah/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparison of Discourse Surrounding CRISPR/Cas9 in the Media and Peer-Reviewed Literature</title>
      <link>https://alisawuffles.github.io/publication/crispr/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://alisawuffles.github.io/publication/crispr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
